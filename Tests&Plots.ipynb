{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb20385-eaf3-44a6-8dc1-6eeff8b74b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import powerplantmatching as pm\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "from scipy.stats import expon\n",
    "from scipy.optimize import curve_fit\n",
    "from astropy.visualization import hist\n",
    "#for fitting:\n",
    "from scipy.stats import expon, rv_discrete\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#for plotting Markov Chain graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#helper functions\n",
    "\n",
    "def import_df(path, cols = ['StartTS', 'EndTS', 'TimeZone', 'Status', 'Type', 'AreaCode',\n",
    "       'AreaTypeCode', 'AreaName', 'MapCode', 'PowerResourceEIC', 'UnitName',\n",
    "       'ProductionType', 'InstalledCapacity', 'AvailableCapacity',\n",
    "       'Reason']):\n",
    "    \"\"\"\n",
    "    imports and preprocess data_frame\n",
    "    path: string containing path of csv file containing table\n",
    "    cols: list of column names to select in df\n",
    "    returns: non redundat dataframe with only failures\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(path, sep = \"\\t\", parse_dates = [0,1])\n",
    "    df = df[cols] #selects only column\n",
    "    df = df.drop_duplicates(subset = [\"UnitName\",\"StartTS\"]) #deletes redundant rows\n",
    "    #df = df[(df[\"Reason\"] == \"Failure\")] # WHERE | (df[\"Reason\"] == 'Foreseen Maintenance')\n",
    "    #maybe can do df[df[\"Reason\"] in reasons]?\n",
    "    return df\n",
    "\n",
    "def get_week(date):\n",
    "    \"\"\"\n",
    "    input: date in date_time format\n",
    "    output: what week of the year the date corresponds to\n",
    "    \"\"\"\n",
    "    return date.week\n",
    "\n",
    "\n",
    "def truncate(number, digits) -> float:\n",
    "    # Improve accuracy with floating point operations, to avoid truncate(16.4, 2) = 16.39 or truncate(-1.13, 2) = -1.12\n",
    "    nbDecimals = len(str(number).split('.')[1]) \n",
    "    if nbDecimals <= digits:\n",
    "        return number\n",
    "    stepper = 10.0 ** digits\n",
    "    return math.trunc(stepper * number) / stepper\n",
    "\n",
    "def markov_graph(transitions, seed = 42, digits = 4, title = \"\"):\n",
    "    \"\"\"\n",
    "    input: transitions, a dictionary having as \n",
    "    keys: touples with 2 elements being the from state and from state\n",
    "    values: the transition probability\n",
    "    output: markov chain graph\n",
    "    \"\"\"\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    for transition, probability in transitions.items():\n",
    "        state_from, state_to = transition\n",
    "        if probability != 0: \n",
    "        #if probability state_from to state_to is not 0 we add an edge to the graph\n",
    "            G.add_edge(state_from, state_to, weight=truncate(probability, digits))\n",
    "\n",
    "    #create positions of nodes: dictionary with coordinates\n",
    "    pos = nx.spring_layout(G, seed) \n",
    "\n",
    "    # Increase the scale to avoid overlap\n",
    "    pos = {k: [v[0] * 2, v[1] * 2] for k, v in pos.items()}\n",
    "\n",
    "    labels = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw(G, pos, with_labels=True, node_size=700, node_color='skyblue', font_size=8, font_color='black',\n",
    "            connectionstyle='arc3,rad=0.1')\n",
    "\n",
    "    # Annotate edges manually with adjusted positions to avoid overlap\n",
    "    for edge, weight in labels.items():\n",
    "        (x, y) = pos[edge[0]]\n",
    "        text_x = 3/4*x + 1/4*pos[edge[1]][0]\n",
    "        text_y = 3/4*y + 1/4*pos[edge[1]][1]\n",
    "        #shift text to avoid overlap\n",
    "        text_y += 0.2 if edge[0] == edge[1] else 0\n",
    "\n",
    "\n",
    "        plt.text(text_x, text_y, f\"{weight}\", fontsize=8, color='blue', verticalalignment='center',\n",
    "                 horizontalalignment='center')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def combine_overlaps(df):\n",
    "    \"\"\"\n",
    "    this functions combines any time overlaps present in the dataframe for each generator\n",
    "    so that for every time t there is at most one row describing the generator at time t.\n",
    "    df: dataframe containing UnitName, StartTS, EndTS\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Sort the DataFrame\n",
    "    df.sort_values(by=[\"UnitName\", \"StartTS\"], inplace=True)\n",
    "\n",
    "    # Step 2 and 3: Combine overlapping intervals\n",
    "    result = []\n",
    "    current_interval = None\n",
    "    n_rows = df.shape[0]\n",
    "    perc = n_rows // 100 *  5\n",
    "\n",
    "    for k, row in df.iterrows():\n",
    "        if k % perc == 0:\n",
    "            print(f\"percentage of rows parsed = {k / n_rows *100:.2f}%\")\n",
    "        if current_interval is None:\n",
    "             current_interval = row.copy()\n",
    "        elif row[\"StartTS\"] >= current_interval[\"EndTS\"] or row[\"UnitName\"] != current_interval[\"UnitName\"]:\n",
    "            # No overlap or new UnitID\n",
    "            result.append(current_interval)\n",
    "            current_interval = row.copy()\n",
    "        else:\n",
    "            # Overlapping intervals, update the EndTS\n",
    "            current_interval[\"EndTS\"] = row[\"StartTS\"]\n",
    "\n",
    "    result_df = pd.DataFrame(result)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def get_markov_probs(df, states_column):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df: dataframe having as columns: states_column, \"ProductionType\", \"StartTS\", \"UpTime\"\n",
    "    states_column: string with name of column where the state of the generator is saved\n",
    "    output: dictionary having as keys tuples with two states and the associated probability transition\n",
    "    \"\"\"\n",
    "    states = list(df[states_column].unique())\n",
    "    states.append(\"Running\")\n",
    "    transitions = []\n",
    "    for x in states:\n",
    "        for y in states:\n",
    "            transitions.append((x,y))\n",
    "            \n",
    "    transitions_counter = dict(zip(transitions, [0]*len(transitions)))\n",
    "    GenGroups = df.groupby(\"UnitName\")\n",
    "    previous_state = \"Running\"\n",
    "    current_state = \"Running\"\n",
    "    for unit_name, unit_df in GenGroups:\n",
    "        unit_df = unit_df.sort_values([\"StartTS\"])\n",
    "        #count transition occurante for unit\n",
    "        for index, row in unit_df.iterrows():\n",
    "            uptime = row[\"RunningTime\"]\n",
    "            #get current state from row\n",
    "            current_state = row[states_column]\n",
    "\n",
    "            if pd.isna(uptime):\n",
    "                #if uptime == \"Nan\" then it was the first recorded instance of the generator in the dataframe so before it was running.\n",
    "                previous_state = \"Running\"\n",
    "            elif uptime > 10 / (60 * 24): # and previous_state != \"Running\"\n",
    "                #if the generator had some time between the previous row than the previous state was running\n",
    "                #and we must add 1 to previousprevious state and running\n",
    "                transitions_counter[(previous_state, \"Running\")] += 1\n",
    "                previous_state = \"Running\"    \n",
    "\n",
    "            transitions_counter[(previous_state, current_state)] += 1\n",
    "            #the current state becomes the previous_state\n",
    "            previous_state = current_state\n",
    "\n",
    "    #get the transtions probabilities\n",
    "    transitions_probs = transitions_counter\n",
    "    counter_dict = dict(zip(states, [0]*len(states)))\n",
    "    for state in states:\n",
    "        for transition, counter in transitions_probs.items():\n",
    "            if transition[0] == state:\n",
    "                counter_dict[state] += counter \n",
    "\n",
    "    for transition, counter in transitions_probs.items():\n",
    "        if counter_dict[transition[0]] != 0:\n",
    "            #if transition[0] occurs at least one time\n",
    "            transitions_probs[transition] = transitions_probs[transition] / counter_dict[transition[0]]\n",
    "    return transitions_probs\n",
    "\n",
    "def weighted_values(values, probabilities, size):\n",
    "    bins = np.add.accumulate(probabilities)\n",
    "    return values[np.digitize(np.random.random_sample(size), bins)]\n",
    "\n",
    "def next_state_markov(markov, current_state):\n",
    "    possible_states = []\n",
    "    transition_probs = []\n",
    "    for key, prob in markov.items():\n",
    "        if key[0] == current_state and prob != 0:\n",
    "            possible_states.append(key[1])\n",
    "            transition_probs.append(prob)\n",
    "    #if len(possible_states) == 0:\n",
    "    #    return current_state\n",
    "    #else:\n",
    "    return weighted_values(np.array(possible_states), np.array(transition_probs),1)[0]\n",
    "\n",
    "def get_gen_type(geo_df):\n",
    "    geo_to_entsoe_gen_d = {\n",
    "        \"Hard Coal\": \"Fossil Hard coal \", \n",
    "        'Lignite' : 'Fossil Brown coal/Lignite ',\n",
    "        'Oil': \"Fossil Oil \",\n",
    "        'Waste': \"Waste \",\n",
    "        'Natural Gas': \"Fossil Gas \",#o ci andrebbe qualcos altro\n",
    "        #'Hydro',\n",
    "        'Nuclear': \"Nuclear \", \n",
    "        'Other' : \"Other \", \n",
    "        'Solar':\"Solar \",\n",
    "        'Wind':\"Wind Onshore \",\n",
    "        'Geothermal':\"Geothermal \"\n",
    "        }\n",
    "    #given a unit row of geo dataframe give gen type in ENTSOE format\n",
    "    #in geo there is just one windpower type (not onshore or offshore) or maybe you can see from the dataframe\n",
    "    fuel_type = geo_df[\"Fueltype\"]\n",
    "    gen_type = geo_df[\"Technology\"]\n",
    "    if fuel_type == \"Hydro\":\n",
    "        if gen_type == \"Reservoir\":\n",
    "            return 'Hydro Water Reservoir '\n",
    "        elif gen_type == \"Run-Of-River\":\n",
    "            return 'Hydro Run-of-river and poundage '\n",
    "        elif gen_type == \"Pumped Storage\":\n",
    "            return 'Hydro Pumped Storage '\n",
    "    elif fuel_type in geo_to_entsoe_gen_d.keys():\n",
    "        return geo_to_entsoe_gen_d[fuel_type]\n",
    "    else:\n",
    "        print(f\"Generetor {gen_type},{fuel_type} not found classfied as Other\")\n",
    "        return \"Other \"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301bdfe-e4b3-4955-9570-473434cf3f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Outages Dataframe to construct model ENTSO-E format\n",
    "data_path = \"../outagesmodelingdata/\"\n",
    "df = pd.read_csv(data_path+\"deltaWithEverything_df.csv\", parse_dates = [0,1])\n",
    "df[\"StartTS\"] = pd.to_datetime(df[\"StartTS\"])\n",
    "df[\"EndTS\"] = pd.to_datetime(df[\"EndTS\"])\n",
    "df = df.sort_values([\"UnitName\",\"StartTS\"])\n",
    "\n",
    "#Combine overlaps fo that every generator is only at one state at the time\n",
    "df = combine_overlaps(df)\n",
    "#combine countries\n",
    "df[\"MapCode\"] = df[\"MapCode\"].apply(lambda x: x[0:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d447a42-7fcf-452d-bb1e-4e171530f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario.set_index(\"TimeStamp\").groupby(\n",
    "    [\n",
    "        pd.Series(scenario.set_index(\"TimeStamp\").columns).map(geo.set_index(\"projectID\").Country).values,\n",
    "        pd.Series(scenario.set_index(\"TimeStamp\").columns).map(geo.set_index(\"projectID\").Fueltype).values,\n",
    "    ],\n",
    "    axis=1\n",
    ").mean()[\"Germany\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb066c3-6da0-40f2-85e9-013112c6746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_series = pd.Series(scenario.columns[1:])\n",
    "\n",
    "scenario.set_index(\"TimeStamp\").groupby(\n",
    "    [\n",
    "        gen_series.map(geo.set_index(\"projectID\").Country).values,\n",
    "        gen_series.map(geo.set_index(\"projectID\").Fueltype).values,\n",
    "    ],\n",
    "    axis=1\n",
    ").mean()[\"Austria\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884cbbd0-3013-4b6e-b831-99f4df6fa529",
   "metadata": {},
   "source": [
    "# Plot Mean running capacity by type of generator over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9345aa26-98b1-4fc6-b8bf-73a08468a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = df.PowerResourceEIC.unique()\n",
    "start_date = df.StartTS.min() + np.timedelta64(24,\"M\")\n",
    "#end_date = df.EndTS.max()\n",
    "end_date = start_date + np.timedelta64(6,\"M\")\n",
    "\n",
    "print(\"generators in germany in dataset:\",np.sum(gen_info[\"MapCode\"] == \"DE\"))\n",
    "print(\"generators in germany in generated scenarios:\", np.sum(geo.Country == \"Germany\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea801563-a59a-48a2-9a96-6a70610a730e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#convert to pypsa format\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#check for first event correctness\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnitName\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStartTS\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m scenarios_d \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gen, gen_df \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnitName\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#convert to pypsa format\n",
    "#check for first event correctness\n",
    "df = df.sort_values([\"UnitName\",\"StartTS\"])\n",
    "scenarios_d = {}\n",
    "for gen, gen_df in df.groupby(\"UnitName\"):\n",
    "    gen_scenario = []\n",
    "    current_date = start_date\n",
    "    #we assume df is sorted by startTS\n",
    "    \n",
    "    for index, event in gen_df.iterrows():\n",
    "        if event.EndTS <= end_date:\n",
    "            hours_before_event = int((event[\"StartTS\"] - current_date) / np.timedelta64(1,\"h\"))\n",
    "            gen_scenario += [1] * hours_before_event\n",
    "            duration_event = int((event[\"EndTS\"] - event[\"StartTS\"]) / np.timedelta64(1,\"h\"))\n",
    "            pu = event[\"p.u.\"]\n",
    "            gen_scenario += [pu] * duration_event\n",
    "            current_date = event[\"EndTS\"]\n",
    "        elif event.StartTS <= end_date:\n",
    "            hours_before_event = int((event[\"StartTS\"] - current_date) / np.timedelta64(1,\"h\"))\n",
    "            gen_scenario += [1] * hours_before_event\n",
    "            duration_event = int((end_date - event[\"StartTS\"]) / np.timedelta64(1,\"h\"))\n",
    "            pu = event[\"p.u.\"]\n",
    "            gen_scenario += [pu] * duration_event\n",
    "            current_date = end_date\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    last_hours = int((end_date - current_date) / np.timedelta64(1,\"h\"))\n",
    "    gen_scenario += [1]*last_hours\n",
    "    scenarios_d[gen] = gen_scenario\n",
    "\n",
    "\n",
    "min_l = 50000\n",
    "for key, value in scenarios_d.items():\n",
    "    if len(value) < min_l:\n",
    "        min_l = len(value)\n",
    "for key, value in scenarios_d.items():\n",
    "    scenarios_d[key] = scenarios_d[key][:min_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2518a8-314d-4805-a1f4-6bd972ab40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = []\n",
    "for h in np.arange(min_l):\n",
    "    timestamp += [start_date + np.timedelta64(h,\"h\")]\n",
    "scenarios_d[\"TimeStamp\"] = timestamp\n",
    "data_scenario = pd.DataFrame(scenarios_d)\n",
    "\n",
    "gen_series = pd.Series(data_scenario.set_index(\"TimeStamp\").columns)\n",
    "gen_info = df.groupby(\"UnitName\").first().reset_index()\n",
    "data_scenario.set_index(\"TimeStamp\").groupby(\n",
    "    [\n",
    "        gen_series.map(gen_info.set_index(\"UnitName\").MapCode).values,\n",
    "        gen_series.map(gen_info.set_index(\"UnitName\").ProductionType).values,\n",
    "    ],\n",
    "    axis=1\n",
    ").mean()[\"DE\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15d70d-9d9a-4f3a-8ef2-a22b8c577e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
