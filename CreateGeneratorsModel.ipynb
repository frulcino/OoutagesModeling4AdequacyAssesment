{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497a6bb4-5f99-4beb-abd3-584f43c1d845",
   "metadata": {},
   "source": [
    "# Model description\n",
    "The maintenance and failure of generators scenario is modeled as a markov chain where at each change of state a random value is gerated to determine how much time each generator spends in the new state and at what maximum capacity.\n",
    "The state transition probabilities are obtained by counting the number of transitions in the given dataset (df) for each possible transition and dividing by the total number of transitions with the same starting state. The time spent at each state is modeled indipendently for each different type of generator by fitting the data with an exponential distribution. The capacity distribution is instead approximated by a kernel density estimator.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset used for modeling it sourced by [ENTSOE](https://transparency.entsoe.eu/outage-domain/r2/unavailabilityOfProductionAndGenerationUnits/show?name=&defaultValue=true&viewType=TABLE&areaType=CTA&atch=false&dateTime.dateTime=28.02.2024+00:00|UTC|DAY&dateTime.endDateTime=01.03.2024+00:00|UTC|DAY&CTY|10YAT-APG------L|MULTI=CTY|10YAT-APG------L|MULTI&area.values=CTY|10YAT-APG------L!CTA|10YAT-APG------L&assetType.values=PU&assetType.values=GU&outageType.values=A54&outageType.values=A53&outageStatus.values=A05&masterDataFilterName=&masterDataFilterCode=&dv-datatable_length=10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0acb7ee8-2a9f-44f2-974b-d562af89c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "\n",
    "def get_week(date):\n",
    "    \"\"\"\n",
    "    input: date in date_time format\n",
    "    output: what week of the year the date corresponds to\n",
    "    \"\"\"\n",
    "    return date.week\n",
    "\n",
    "\n",
    "def truncate(number, digits) -> float:\n",
    "    # Improve accuracy with floating point operations, to avoid truncate(16.4, 2) = 16.39 or truncate(-1.13, 2) = -1.12\n",
    "    nbDecimals = len(str(number).split('.')[1]) \n",
    "    if nbDecimals <= digits:\n",
    "        return number\n",
    "    stepper = 10.0 ** digits\n",
    "    return math.trunc(stepper * number) / stepper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8505856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import powerplantmatching as pm\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "from scipy.stats import expon\n",
    "from scipy.optimize import curve_fit\n",
    "from astropy.visualization import hist\n",
    "#for fitting:\n",
    "from scipy.stats import expon, rv_discrete\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#for plotting Markov Chain graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#helper functions\n",
    "\n",
    "def import_df(path, cols = ['StartTS', 'EndTS', 'TimeZone', 'Status', 'Type', 'AreaCode',\n",
    "       'AreaTypeCode', 'AreaName', 'MapCode', 'PowerResourceEIC', 'UnitName',\n",
    "       'ProductionType', 'InstalledCapacity', 'AvailableCapacity',\n",
    "       'Reason']):\n",
    "    \"\"\"\n",
    "    imports and preprocess data_frame\n",
    "    path: string containing path of csv file containing table\n",
    "    cols: list of column names to select in df\n",
    "    returns: non redundat dataframe with only failures\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(path, sep = \"\\t\", parse_dates = [0,1])\n",
    "    df = df[cols] #selects only column\n",
    "    df = df.drop_duplicates(subset = [\"UnitName\",\"StartTS\"]) #deletes redundant rows\n",
    "    #df = df[(df[\"Reason\"] == \"Failure\")] # WHERE | (df[\"Reason\"] == 'Foreseen Maintenance')\n",
    "    #maybe can do df[df[\"Reason\"] in reasons]?\n",
    "    return df\n",
    "\n",
    "\n",
    "def markov_graph(transitions, seed = 42, digits = 4, title = \"\"):\n",
    "    \"\"\"\n",
    "    input: transitions, a dictionary having as \n",
    "    keys: touples with 2 elements being the from state and from state\n",
    "    values: the transition probability\n",
    "    output: markov chain graph\n",
    "    \"\"\"\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    for transition, probability in transitions.items():\n",
    "        state_from, state_to = transition\n",
    "        if probability != 0: \n",
    "        #if probability state_from to state_to is not 0 we add an edge to the graph\n",
    "            G.add_edge(state_from, state_to, weight=truncate(probability, digits))\n",
    "\n",
    "    #create positions of nodes: dictionary with coordinates\n",
    "    pos = nx.spring_layout(G, seed) \n",
    "\n",
    "    # Increase the scale to avoid overlap\n",
    "    pos = {k: [v[0] * 2, v[1] * 2] for k, v in pos.items()}\n",
    "\n",
    "    labels = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw(G, pos, with_labels=True, node_size=700, node_color='skyblue', font_size=8, font_color='black',\n",
    "            connectionstyle='arc3,rad=0.1')\n",
    "\n",
    "    # Annotate edges manually with adjusted positions to avoid overlap\n",
    "    for edge, weight in labels.items():\n",
    "        (x, y) = pos[edge[0]]\n",
    "        text_x = 3/4*x + 1/4*pos[edge[1]][0]\n",
    "        text_y = 3/4*y + 1/4*pos[edge[1]][1]\n",
    "        #shift text to avoid overlap\n",
    "        text_y += 0.2 if edge[0] == edge[1] else 0\n",
    "\n",
    "\n",
    "        plt.text(text_x, text_y, f\"{weight}\", fontsize=8, color='blue', verticalalignment='center',\n",
    "                 horizontalalignment='center')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def combine_overlaps(df):\n",
    "    \"\"\"\n",
    "    this functions combines any time overlaps present in the dataframe for each generator\n",
    "    so that for every time t there is at most one row describing the generator at time t.\n",
    "    df: dataframe containing UnitName, StartTS, EndTS\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Sort the DataFrame\n",
    "    df.sort_values(by=[\"UnitName\", \"StartTS\"], inplace=True)\n",
    "\n",
    "    # Step 2 and 3: Combine overlapping intervals\n",
    "    result = []\n",
    "    current_interval = None\n",
    "    n_rows = df.shape[0]\n",
    "    perc = n_rows // 100 *  5\n",
    "\n",
    "    for k, row in df.iterrows():\n",
    "        if k % perc == 0:\n",
    "            print(f\"percentage of rows parsed = {k / n_rows *100:.2f}%\")\n",
    "        if current_interval is None:\n",
    "             current_interval = row.copy()\n",
    "        elif row[\"StartTS\"] >= current_interval[\"EndTS\"] or row[\"UnitName\"] != current_interval[\"UnitName\"]:\n",
    "            # No overlap or new UnitID\n",
    "            result.append(current_interval)\n",
    "            current_interval = row.copy()\n",
    "        else:\n",
    "            # Overlapping intervals, update the EndTS\n",
    "            current_interval[\"EndTS\"] = row[\"StartTS\"]\n",
    "\n",
    "    result_df = pd.DataFrame(result)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def get_markov_probs(df, states_column):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df: dataframe having as columns: states_column, \"ProductionType\", \"StartTS\", \"UpTime\"\n",
    "    states_column: string with name of column where the state of the generator is saved\n",
    "    output: dictionary having as keys tuples with two states and the associated probability transition\n",
    "    \"\"\"\n",
    "    states = list(df[states_column].unique())\n",
    "    states.append(\"Running\")\n",
    "    transitions = []\n",
    "    for x in states:\n",
    "        for y in states:\n",
    "            transitions.append((x,y))\n",
    "            \n",
    "    transitions_counter = dict(zip(transitions, [0]*len(transitions)))\n",
    "    GenGroups = df.groupby(\"UnitName\")\n",
    "    previous_state = \"Running\"\n",
    "    current_state = \"Running\"\n",
    "    for unit_name, unit_df in GenGroups:\n",
    "        unit_df = unit_df.sort_values([\"StartTS\"])\n",
    "        #count transition occurante for unit\n",
    "        for index, row in unit_df.iterrows():\n",
    "            uptime = row[\"RunningTime\"]\n",
    "            #get current state from row\n",
    "            current_state = row[states_column]\n",
    "\n",
    "            if pd.isna(uptime):\n",
    "                #if uptime == \"Nan\" then it was the first recorded instance of the generator in the dataframe so before it was running.\n",
    "                previous_state = \"Running\"\n",
    "            elif uptime > 10 / (60 * 24): # and previous_state != \"Running\"\n",
    "                #if the generator had some time between the previous row than the previous state was running\n",
    "                #and we must add 1 to previousprevious state and running\n",
    "                transitions_counter[(previous_state, \"Running\")] += 1\n",
    "                previous_state = \"Running\"    \n",
    "\n",
    "            transitions_counter[(previous_state, current_state)] += 1\n",
    "            #the current state becomes the previous_state\n",
    "            previous_state = current_state\n",
    "\n",
    "    #get the transtions probabilities\n",
    "    transitions_probs = transitions_counter\n",
    "    counter_dict = dict(zip(states, [0]*len(states)))\n",
    "    for state in states:\n",
    "        for transition, counter in transitions_probs.items():\n",
    "            if transition[0] == state:\n",
    "                counter_dict[state] += counter \n",
    "\n",
    "    for transition, counter in transitions_probs.items():\n",
    "        if counter_dict[transition[0]] != 0:\n",
    "            #if transition[0] occurs at least one time\n",
    "            transitions_probs[transition] = transitions_probs[transition] / counter_dict[transition[0]]\n",
    "    return transitions_probs\n",
    "\n",
    "def weighted_values(values, probabilities, size):\n",
    "    bins = np.add.accumulate(probabilities)\n",
    "    return values[np.digitize(np.random.random_sample(size), bins)]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b535ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of rows parsed = 0.00%\n",
      "percentage of rows parsed = 5.00%\n",
      "percentage of rows parsed = 10.00%\n",
      "percentage of rows parsed = 15.00%\n",
      "percentage of rows parsed = 20.00%\n",
      "percentage of rows parsed = 25.00%\n",
      "percentage of rows parsed = 30.00%\n",
      "percentage of rows parsed = 35.00%\n",
      "percentage of rows parsed = 39.99%\n",
      "percentage of rows parsed = 44.99%\n",
      "percentage of rows parsed = 49.99%\n",
      "percentage of rows parsed = 54.99%\n",
      "percentage of rows parsed = 59.99%\n",
      "percentage of rows parsed = 64.99%\n",
      "percentage of rows parsed = 69.99%\n",
      "percentage of rows parsed = 74.99%\n",
      "percentage of rows parsed = 79.99%\n",
      "percentage of rows parsed = 84.99%\n",
      "percentage of rows parsed = 89.99%\n",
      "percentage of rows parsed = 94.99%\n",
      "percentage of rows parsed = 99.99%\n"
     ]
    }
   ],
   "source": [
    "#Import Outages Dataframe to construct model ENTSO-E format\n",
    "data_path = \"../outagesmodelingdata/\"\n",
    "df = pd.read_csv(data_path+\"deltaWithEverything_df.csv\", parse_dates = [0,1])\n",
    "df[\"StartTS\"] = pd.to_datetime(df[\"StartTS\"])\n",
    "df[\"EndTS\"] = pd.to_datetime(df[\"EndTS\"])\n",
    "df = df.sort_values([\"UnitName\",\"StartTS\"])\n",
    "\n",
    "#Combines overlaps so that every generator is only at one state at the time\n",
    "df = combine_overlaps(df)\n",
    "#combine regions of countries into correspondint country name\n",
    "df[\"MapCode\"] = df[\"MapCode\"].apply(lambda x: x[0:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f71e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Markov chain modeling\n",
      "Starting Capacity Modeling\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#statetime_df: dataframe with the parametres of the exponantial distribution of statetime\n",
    "#markov_d: dictionary of markov chains of the various generator types\n",
    "#capacity_d: dictionary of distributions of capacity in p.u.\n",
    "\n",
    "\n",
    "#STATE TIME MODELING\n",
    "\n",
    "\n",
    "#Change state_column to look at different state distributions\n",
    "state_column = \"Type\"\n",
    "#def state_time_distribution_fitting(df, state_column):\n",
    "states = list(df[state_column].unique())\n",
    "delta_df = df\n",
    "delta_df = delta_df.sort_values(by = [\"UnitName\", \"StartTS\"])\n",
    "\n",
    "for state in states:\n",
    "    #calculate time spent in each state\n",
    "    delta_df[state+\"Time\"] = [np.datetime64(\"NaT\")]*df.shape[0]\n",
    "    state_df = delta_df[delta_df[state_column] == state]\n",
    "    delta_df.loc[delta_df[state_column] == state, state+\"Time\"] = state_df[\"EndTS\"] - state_df[\"StartTS\"]\n",
    "    delta_df[state+\"Time\"] = (delta_df[state+\"Time\"] /  np.timedelta64(1, 'h'))\n",
    "    \n",
    "for unit, unit_df in delta_df.groupby(\"UnitName\"):                             \n",
    "    unit_df[\"EndTS\"] = pd.to_datetime(unit_df[\"EndTS\"])\n",
    "    shifted_endts = pd.to_datetime(unit_df[\"EndTS\"].shift())\n",
    "    start_ts = delta_df.loc[delta_df[\"UnitName\"] == unit, \"StartTS\"]\n",
    "    delta_df.loc[delta_df[\"UnitName\"] == unit, \"RunningTime\"] = start_ts - shifted_endts\n",
    "\n",
    "delta_df[\"RunningTime\"] = (delta_df[\"RunningTime\"] /  np.timedelta64(1, 'h'))\n",
    "delta_df.loc[delta_df[\"RunningTime\"] == 0, \"RunningTime\"] = np.nan\n",
    "\n",
    "states = states + [\"Running\"]\n",
    "\n",
    "\n",
    "#fit outages distributions\n",
    "statetime_df = pd.DataFrame() #create empty parameter table\n",
    "grouped_delta = delta_df.groupby([\"ProductionType\"])\n",
    "statetime_df[\"ProductionType\"] = grouped_delta.first().reset_index()[\"ProductionType\"]\n",
    "for state in states:\n",
    "    statetime_df[state + \"Time\"] = [np.nan]*len(list(statetime_df[\"ProductionType\"]))\n",
    "\n",
    "\n",
    "def exponential_fit(x, scale):\n",
    "    return expon.pdf(x, scale=scale)\n",
    "\n",
    "\n",
    "for group_name, group_df in grouped_delta:\n",
    "    for state in states:\n",
    "        if not pd.isna(group_df[state+\"Time\"].mean()):\n",
    "            # Fit the data to the exponential function\n",
    "            mean = group_df[state+\"Time\"].mean()\n",
    "            state_scale = mean #this is the MLE for exponential distribution\n",
    "            statetime_df.loc[statetime_df[\"ProductionType\"] == group_name[0], state+\"Time\"] = state_scale\n",
    "\n",
    "\n",
    "            \n",
    "#MARKOV CHAIN MODELING\n",
    "print(\"Starting Markov chain modeling\")\n",
    "df = delta_df[delta_df[\"Reason\"] != \"Shutdown\" ] #remove shutdowns \n",
    "\n",
    "markov_d= {}\n",
    "#Create a list containing tuples rapresenting all possible state changes (x,y) := x --> y\n",
    "#data is a dataframe containing the correct \"UpTime\" between the states considered\n",
    "\n",
    "GenTypeGroup_df = df.groupby(\"ProductionType\")\n",
    "\n",
    "#We can use different states\n",
    "#print markov chain for each type of generator\n",
    "for production_type, data in GenTypeGroup_df:\n",
    "    transitions_probs = get_markov_probs(data, \"Type\")\n",
    "    markov_d[production_type] = transitions_probs\n",
    "    #uncomment to plot:\n",
    "    #markov_graph(transitions_probs, title = f\"{production_type} Markov Chain\")\n",
    "\n",
    "#t_probs = dict(transitions_probs)\n",
    "#for key, value in transitions_probs.items():\n",
    "#    if value < 0.05:\n",
    "#        del t_probs[key]\n",
    "#print graph\n",
    "\n",
    "\n",
    "#CAPACITY MODELING - the parameters for KDE correspond to the input data.\n",
    "print(\"Starting Capacity Modeling\")\n",
    "df[\"p.u.\"] = df[\"AvailableCapacity\"].copy() / df[\"InstalledCapacity\"]\n",
    "df = df[~np.isnan(df[\"pu\"])]\n",
    "kernel_df = df[[\"ProductionType\", \"Type\", \"p.u.\"]]\n",
    "\n",
    "#SAVE CALCULATED PARAMETERS:\n",
    "kernel_df.to_csv(\"kernel_capacity_df.csv\", index = False)\n",
    "statetime_df.to_csv(\"exponential_statetime_df.csv\", index = False)\n",
    "np.save(\"markov_state_change_d.npy\",markov_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c81af1-b1c7-4a9b-bfb0-26698ca9f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save models\n",
    "kernel_df.to_csv(\"kernel_capacity_df.csv\")\n",
    "statetime_df.to_csv(\"exponential_statetime_df.csv\")\n",
    "np.save(\"markov_state_change_d.npy\",markov_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380de1e5-9d57-454f-adca-3a8c0591b3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
