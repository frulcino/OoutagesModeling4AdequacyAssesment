{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ebcd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import powerplantmatching as pm\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "from scipy.stats import expon\n",
    "from scipy.optimize import curve_fit\n",
    "from astropy.visualization import hist\n",
    "#for fitting:\n",
    "from scipy.stats import expon, rv_discrete\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#for plotting Markov Chain graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#helper functions\n",
    "\n",
    "def import_df(path, cols = ['StartTS', 'EndTS', 'TimeZone', 'Status', 'Type', 'AreaCode',\n",
    "       'AreaTypeCode', 'AreaName', 'MapCode', 'PowerResourceEIC', 'UnitName',\n",
    "       'ProductionType', 'InstalledCapacity', 'AvailableCapacity',\n",
    "       'Reason']):\n",
    "    \"\"\"\n",
    "    imports and preprocess data_frame\n",
    "    path: string containing path of csv file containing table\n",
    "    cols: list of column names to select in df\n",
    "    returns: non redundat dataframe with only failures\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(path, sep = \"\\t\", parse_dates = [0,1])\n",
    "    df = df[cols] #selects only column\n",
    "    df = df.drop_duplicates(subset = [\"UnitName\",\"StartTS\"]) #deletes redundant rows\n",
    "    #df = df[(df[\"Reason\"] == \"Failure\")] # WHERE | (df[\"Reason\"] == 'Foreseen Maintenance')\n",
    "    #maybe can do df[df[\"Reason\"] in reasons]?\n",
    "    return df\n",
    "\n",
    "def get_week(date):\n",
    "    \"\"\"\n",
    "    input: date in date_time format\n",
    "    output: what week of the year the date corresponds to\n",
    "    \"\"\"\n",
    "    return date.week\n",
    "\n",
    "\n",
    "def truncate(number, digits) -> float:\n",
    "    # Improve accuracy with floating point operations, to avoid truncate(16.4, 2) = 16.39 or truncate(-1.13, 2) = -1.12\n",
    "    nbDecimals = len(str(number).split('.')[1]) \n",
    "    if nbDecimals <= digits:\n",
    "        return number\n",
    "    stepper = 10.0 ** digits\n",
    "    return math.trunc(stepper * number) / stepper\n",
    "\n",
    "def markov_graph(transitions, seed = 42, digits = 4, title = \"\"):\n",
    "    \"\"\"\n",
    "    input: transitions, a dictionary having as \n",
    "    keys: touples with 2 elements being the from state and from state\n",
    "    values: the transition probability\n",
    "    output: markov chain graph\n",
    "    \"\"\"\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    for transition, probability in transitions.items():\n",
    "        state_from, state_to = transition\n",
    "        if probability != 0: \n",
    "        #if probability state_from to state_to is not 0 we add an edge to the graph\n",
    "            G.add_edge(state_from, state_to, weight=truncate(probability, digits))\n",
    "\n",
    "    #create positions of nodes: dictionary with coordinates\n",
    "    pos = nx.spring_layout(G, seed) \n",
    "\n",
    "    # Increase the scale to avoid overlap\n",
    "    pos = {k: [v[0] * 2, v[1] * 2] for k, v in pos.items()}\n",
    "\n",
    "    labels = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw(G, pos, with_labels=True, node_size=700, node_color='skyblue', font_size=8, font_color='black',\n",
    "            connectionstyle='arc3,rad=0.1')\n",
    "\n",
    "    # Annotate edges manually with adjusted positions to avoid overlap\n",
    "    for edge, weight in labels.items():\n",
    "        (x, y) = pos[edge[0]]\n",
    "        text_x = 3/4*x + 1/4*pos[edge[1]][0]\n",
    "        text_y = 3/4*y + 1/4*pos[edge[1]][1]\n",
    "        #shift text to avoid overlap\n",
    "        text_y += 0.2 if edge[0] == edge[1] else 0\n",
    "\n",
    "\n",
    "        plt.text(text_x, text_y, f\"{weight}\", fontsize=8, color='blue', verticalalignment='center',\n",
    "                 horizontalalignment='center')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def combine_overlaps(df):\n",
    "    \"\"\"\n",
    "    this functions combines any time overlaps present in the dataframe for each generator\n",
    "    so that for every time t there is at most one row describing the generator at time t.\n",
    "    df: dataframe containing UnitName, StartTS, EndTS\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Sort the DataFrame\n",
    "    df.sort_values(by=[\"UnitName\", \"StartTS\"], inplace=True)\n",
    "\n",
    "    # Step 2 and 3: Combine overlapping intervals\n",
    "    result = []\n",
    "    current_interval = None\n",
    "    n_rows = df.shape[0]\n",
    "    perc = n_rows // 100 *  5\n",
    "\n",
    "    for k, row in df.iterrows():\n",
    "        if k % perc == 0:\n",
    "            print(f\"percentage of rows parsed = {k / n_rows *100:.2f}%\")\n",
    "        if current_interval is None:\n",
    "             current_interval = row.copy()\n",
    "        elif row[\"StartTS\"] >= current_interval[\"EndTS\"] or row[\"UnitName\"] != current_interval[\"UnitName\"]:\n",
    "            # No overlap or new UnitID\n",
    "            result.append(current_interval)\n",
    "            current_interval = row.copy()\n",
    "        else:\n",
    "            # Overlapping intervals, update the EndTS\n",
    "            current_interval[\"EndTS\"] = row[\"StartTS\"]\n",
    "\n",
    "    result_df = pd.DataFrame(result)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def get_markov_probs(df, states_column):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df: dataframe having as columns: states_column, \"ProductionType\", \"StartTS\", \"UpTime\"\n",
    "    states_column: string with name of column where the state of the generator is saved\n",
    "    output: dictionary having as keys tuples with two states and the associated probability transition\n",
    "    \"\"\"\n",
    "    states = list(df[states_column].unique())\n",
    "    states.append(\"Running\")\n",
    "    transitions = []\n",
    "    for x in states:\n",
    "        for y in states:\n",
    "            transitions.append((x,y))\n",
    "            \n",
    "    transitions_counter = dict(zip(transitions, [0]*len(transitions)))\n",
    "    GenGroups = df.groupby(\"UnitName\")\n",
    "    previous_state = \"Running\"\n",
    "    current_state = \"Running\"\n",
    "    for unit_name, unit_df in GenGroups:\n",
    "        unit_df = unit_df.sort_values([\"StartTS\"])\n",
    "        #count transition occurante for unit\n",
    "        for index, row in unit_df.iterrows():\n",
    "            uptime = row[\"RunningTime\"]\n",
    "            #get current state from row\n",
    "            current_state = row[states_column]\n",
    "\n",
    "            if pd.isna(uptime):\n",
    "                #if uptime == \"Nan\" then it was the first recorded instance of the generator in the dataframe so before it was running.\n",
    "                previous_state = \"Running\"\n",
    "            elif uptime > 10 / (60 * 24): # and previous_state != \"Running\"\n",
    "                #if the generator had some time between the previous row than the previous state was running\n",
    "                #and we must add 1 to previousprevious state and running\n",
    "                transitions_counter[(previous_state, \"Running\")] += 1\n",
    "                previous_state = \"Running\"    \n",
    "\n",
    "            transitions_counter[(previous_state, current_state)] += 1\n",
    "            #the current state becomes the previous_state\n",
    "            previous_state = current_state\n",
    "\n",
    "    #get the transtions probabilities\n",
    "    transitions_probs = transitions_counter\n",
    "    counter_dict = dict(zip(states, [0]*len(states)))\n",
    "    for state in states:\n",
    "        for transition, counter in transitions_probs.items():\n",
    "            if transition[0] == state:\n",
    "                counter_dict[state] += counter \n",
    "\n",
    "    for transition, counter in transitions_probs.items():\n",
    "        if counter_dict[transition[0]] != 0:\n",
    "            #if transition[0] occurs at least one time\n",
    "            transitions_probs[transition] = transitions_probs[transition] / counter_dict[transition[0]]\n",
    "    return transitions_probs\n",
    "\n",
    "def weighted_values(values, probabilities, size):\n",
    "    bins = np.add.accumulate(probabilities)\n",
    "    return values[np.digitize(np.random.random_sample(size), bins)]\n",
    "\n",
    "def next_state_markov(markov, current_state):\n",
    "    possible_states = []\n",
    "    transition_probs = []\n",
    "    for key, prob in markov.items():\n",
    "        if key[0] == current_state and prob != 0:\n",
    "            possible_states.append(key[1])\n",
    "            transition_probs.append(prob)\n",
    "    #if len(possible_states) == 0:\n",
    "    #    return current_state\n",
    "    #else:\n",
    "    return weighted_values(np.array(possible_states), np.array(transition_probs),1)[0]\n",
    "\n",
    "def get_gen_type(geo_df):\n",
    "    geo_to_entsoe_gen_d = {\n",
    "        \"Hard Coal\": \"Fossil Hard coal \", \n",
    "        'Lignite' : 'Fossil Brown coal/Lignite ',\n",
    "        'Oil': \"Fossil Oil \",\n",
    "        'Waste': \"Waste \",\n",
    "        'Natural Gas': \"Fossil Gas \",#o ci andrebbe qualcos altro\n",
    "        #'Hydro',\n",
    "        'Nuclear': \"Nuclear \", \n",
    "        'Other' : \"Other \", \n",
    "        'Solar':\"Solar \",\n",
    "        'Wind':\"Wind Onshore \",\n",
    "        'Geothermal':\"Geothermal \"\n",
    "        }\n",
    "    #given a unit row of geo dataframe give gen type in ENTSOE format\n",
    "    #in geo there is just one windpower type (not onshore or offshore) or maybe you can see from the dataframe\n",
    "    fuel_type = geo_df[\"Fueltype\"]\n",
    "    gen_type = geo_df[\"Technology\"]\n",
    "    if fuel_type == \"Hydro\":\n",
    "        if gen_type == \"Reservoir\":\n",
    "            return 'Hydro Water Reservoir '\n",
    "        elif gen_type == \"Run-Of-River\":\n",
    "            return 'Hydro Run-of-river and poundage '\n",
    "        elif gen_type == \"Pumped Storage\":\n",
    "            return 'Hydro Pumped Storage '\n",
    "    elif fuel_type in geo_to_entsoe_gen_d.keys():\n",
    "        return geo_to_entsoe_gen_d[fuel_type]\n",
    "    else:\n",
    "        print(f\"Generetor {gen_type},{fuel_type} not found classfied as Other\")\n",
    "        return \"Other \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9005890",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "1. There is no onshore / offshore windpower\n",
    "2. What is Hydro without further specification\n",
    "3. For what timeframe should I generate data?\n",
    "\n",
    "# TODO:\n",
    "- ~remove redundant gens in geo~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ee6f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set parameters\n",
    "geo = pm.data.GEO()\n",
    "geo = geo.groupby(\"projectID\").head(1)\n",
    "\n",
    "start_time = np.datetime64(\"2023-01-01T00:00:00\")\n",
    "end_time = np.datetime64(\"2023-06-01T00:00:00\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19459a7",
   "metadata": {},
   "source": [
    "## Dataset descripiton\n",
    "\n",
    "    - projectID - Immutable identifier of the power plant\n",
    "        \n",
    "    - Power plant name - claim of each database\n",
    "\n",
    "    - Fueltype - {Bioenergy, Geothermal, Hard Coal, Hydro, Lignite, Nuclear, Natural Gas, Oil, Solar, Wind, Other}\n",
    "\n",
    "    - Technology - {CCGT, OCGT, Steam Turbine, Combustion Engine, Run-Of-River, Pumped Storage, Reservoir}\n",
    "\n",
    "    - Set - {Power Plant (PP), Combined Heat and Power (CHP), Storages (Stores)}\n",
    "\n",
    "    - Capacity - [MW]\n",
    "\n",
    "    - Duration - Maximum state of charge capacity in terms of hours at full output capacity\n",
    "\n",
    "    - Dam Information - Dam volume [Mm^3] and Dam Height [m]\n",
    "\n",
    "    - Geo-position - Latitude, Longitude\n",
    "\n",
    "    - Country - EU-27 + CH + NO (+ UK) minus Cyprus and Malta\n",
    "\n",
    "    - YearCommissioned - Commmisioning year of the powerplant\n",
    "\n",
    "    - RetroFit - Year of last retrofit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6f0a057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of rows parsed = 0.00%\n",
      "percentage of rows parsed = 1.00%\n",
      "percentage of rows parsed = 2.00%\n",
      "percentage of rows parsed = 3.00%\n",
      "percentage of rows parsed = 4.00%\n",
      "percentage of rows parsed = 5.00%\n",
      "percentage of rows parsed = 6.00%\n",
      "percentage of rows parsed = 7.00%\n",
      "percentage of rows parsed = 8.00%\n",
      "percentage of rows parsed = 9.00%\n",
      "percentage of rows parsed = 10.00%\n",
      "percentage of rows parsed = 11.00%\n",
      "percentage of rows parsed = 12.00%\n",
      "percentage of rows parsed = 13.00%\n",
      "percentage of rows parsed = 14.00%\n",
      "percentage of rows parsed = 15.00%\n",
      "percentage of rows parsed = 16.00%\n",
      "percentage of rows parsed = 17.00%\n",
      "percentage of rows parsed = 18.00%\n",
      "percentage of rows parsed = 19.00%\n",
      "percentage of rows parsed = 20.00%\n",
      "percentage of rows parsed = 21.00%\n",
      "percentage of rows parsed = 22.00%\n",
      "percentage of rows parsed = 23.00%\n",
      "percentage of rows parsed = 24.00%\n",
      "percentage of rows parsed = 25.00%\n",
      "percentage of rows parsed = 26.00%\n",
      "percentage of rows parsed = 27.00%\n",
      "percentage of rows parsed = 28.00%\n",
      "percentage of rows parsed = 29.00%\n",
      "percentage of rows parsed = 30.00%\n",
      "percentage of rows parsed = 31.00%\n",
      "percentage of rows parsed = 32.00%\n",
      "percentage of rows parsed = 33.00%\n",
      "percentage of rows parsed = 34.00%\n",
      "percentage of rows parsed = 35.00%\n",
      "percentage of rows parsed = 36.00%\n",
      "percentage of rows parsed = 37.00%\n",
      "percentage of rows parsed = 38.00%\n",
      "percentage of rows parsed = 38.99%\n",
      "percentage of rows parsed = 39.99%\n",
      "percentage of rows parsed = 40.99%\n",
      "percentage of rows parsed = 41.99%\n",
      "percentage of rows parsed = 42.99%\n",
      "percentage of rows parsed = 43.99%\n",
      "percentage of rows parsed = 44.99%\n",
      "percentage of rows parsed = 45.99%\n",
      "percentage of rows parsed = 46.99%\n",
      "percentage of rows parsed = 47.99%\n",
      "percentage of rows parsed = 48.99%\n",
      "percentage of rows parsed = 49.99%\n",
      "percentage of rows parsed = 50.99%\n",
      "percentage of rows parsed = 51.99%\n",
      "percentage of rows parsed = 52.99%\n",
      "percentage of rows parsed = 53.99%\n",
      "percentage of rows parsed = 54.99%\n",
      "percentage of rows parsed = 55.99%\n",
      "percentage of rows parsed = 56.99%\n",
      "percentage of rows parsed = 57.99%\n",
      "percentage of rows parsed = 58.99%\n",
      "percentage of rows parsed = 59.99%\n",
      "percentage of rows parsed = 60.99%\n",
      "percentage of rows parsed = 61.99%\n",
      "percentage of rows parsed = 62.99%\n",
      "percentage of rows parsed = 63.99%\n",
      "percentage of rows parsed = 64.99%\n",
      "percentage of rows parsed = 65.99%\n",
      "percentage of rows parsed = 66.99%\n",
      "percentage of rows parsed = 67.99%\n",
      "percentage of rows parsed = 68.99%\n",
      "percentage of rows parsed = 69.99%\n",
      "percentage of rows parsed = 70.99%\n",
      "percentage of rows parsed = 71.99%\n",
      "percentage of rows parsed = 72.99%\n",
      "percentage of rows parsed = 73.99%\n",
      "percentage of rows parsed = 74.99%\n",
      "percentage of rows parsed = 75.99%\n",
      "percentage of rows parsed = 76.99%\n",
      "percentage of rows parsed = 77.99%\n",
      "percentage of rows parsed = 78.99%\n",
      "percentage of rows parsed = 79.99%\n",
      "percentage of rows parsed = 80.99%\n",
      "percentage of rows parsed = 81.99%\n",
      "percentage of rows parsed = 82.99%\n",
      "percentage of rows parsed = 83.99%\n",
      "percentage of rows parsed = 84.99%\n",
      "percentage of rows parsed = 85.99%\n",
      "percentage of rows parsed = 86.99%\n",
      "percentage of rows parsed = 87.99%\n",
      "percentage of rows parsed = 88.99%\n",
      "percentage of rows parsed = 89.99%\n",
      "percentage of rows parsed = 90.99%\n",
      "percentage of rows parsed = 91.99%\n",
      "percentage of rows parsed = 92.99%\n",
      "percentage of rows parsed = 93.99%\n",
      "percentage of rows parsed = 94.99%\n",
      "percentage of rows parsed = 95.99%\n",
      "percentage of rows parsed = 96.99%\n",
      "percentage of rows parsed = 97.99%\n",
      "percentage of rows parsed = 98.99%\n",
      "percentage of rows parsed = 99.99%\n"
     ]
    }
   ],
   "source": [
    "#Import Outages Dataframe to construct model ENTSO-E format\n",
    "data_path = \"../outagesmodelingdata/\"\n",
    "df = pd.read_csv(data_path+\"deltaWithEverything_df.csv\", parse_dates = [0,1])\n",
    "df[\"StartTS\"] = pd.to_datetime(df[\"StartTS\"])\n",
    "df[\"EndTS\"] = pd.to_datetime(df[\"EndTS\"])\n",
    "df = df.sort_values([\"UnitName\",\"StartTS\"])\n",
    "\n",
    "#Combine overlaps fo that every generator is only at one state at the time\n",
    "df = combine_overlaps(df)\n",
    "#combine countries\n",
    "df[\"MapCode\"] = df[\"MapCode\"].apply(lambda x: x[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd5e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model\n",
    "#statetime_df: dataframe with the parametres of the exponantialdistribution of statetime\n",
    "#markov_d: dictionary of markov chains of the various generator types\n",
    "#capacity_d: dictionary of distributions of capacity in p.u.\n",
    "#STATE TIME MODELING\n",
    "\n",
    "#In this section we model the time a generator spends in a certain state. We model the random variable time in each state with a exponential distribution using the maximum likelyhood estimator (MLE).\n",
    "\n",
    "#Change state_column to look ad different state distributions\n",
    "state_column = \"Type\"\n",
    "\n",
    "#def state_time_distribution_fitting(df, state_column):\n",
    "\n",
    "states = list(df[state_column].unique())\n",
    "delta_df = df\n",
    "delta_df = delta_df.sort_values(by = [\"UnitName\", \"StartTS\"])\n",
    "\n",
    "for state in states:\n",
    "    #calculate time spend in each state\n",
    "    delta_df[state+\"Time\"] = [np.datetime64(\"NaT\")]*df.shape[0]\n",
    "    state_df = delta_df[delta_df[state_column] == state]\n",
    "    delta_df.loc[delta_df[state_column] == state, state+\"Time\"] = state_df[\"EndTS\"] - state_df[\"StartTS\"]\n",
    "    delta_df[state+\"Time\"] = (delta_df[state+\"Time\"] /  np.timedelta64(1, 'h'))\n",
    "    \n",
    "for unit, unit_df in delta_df.groupby(\"UnitName\"):                             \n",
    "    unit_df[\"EndTS\"] = pd.to_datetime(unit_df[\"EndTS\"])\n",
    "    shifted_endts = pd.to_datetime(unit_df[\"EndTS\"].shift())\n",
    "    start_ts = delta_df.loc[delta_df[\"UnitName\"] == unit, \"StartTS\"]\n",
    "    delta_df.loc[delta_df[\"UnitName\"] == unit, \"RunningTime\"] = start_ts - shifted_endts\n",
    "\n",
    "delta_df[\"RunningTime\"] = (delta_df[\"RunningTime\"] /  np.timedelta64(1, 'h'))\n",
    "delta_df.loc[delta_df[\"RunningTime\"] == 0, \"RunningTime\"] = np.nan\n",
    "\n",
    "states = states + [\"Running\"]\n",
    "\n",
    "\n",
    "#fit outages distributions\n",
    "statetime_df = pd.DataFrame() #create empty parameter table\n",
    "grouped_delta = delta_df.groupby([\"ProductionType\"])\n",
    "statetime_df[\"ProductionType\"] = grouped_delta.first().reset_index()[\"ProductionType\"]\n",
    "for state in states:\n",
    "    statetime_df[state + \"Time\"] = [np.nan]*len(list(statetime_df[\"ProductionType\"]))\n",
    "\n",
    "\n",
    "def exponential_fit(x, scale):\n",
    "    return expon.pdf(x, scale=scale)\n",
    "\n",
    "\n",
    "for group_name, group_df in grouped_delta:\n",
    "    \n",
    "    #We drop NaN valued rows\n",
    "    #group_df = group_df.dropna(subset = [\"UpTime\",\"OffTime\"])\n",
    "    \n",
    "    for state in states:\n",
    "        if not pd.isna(group_df[state+\"Time\"].mean()):\n",
    "            # Fit the data to the exponential function\n",
    "            mean = group_df[state+\"Time\"].mean()\n",
    "            state_scale = mean\n",
    "            statetime_df.loc[statetime_df[\"ProductionType\"] == group_name[0], state+\"Time\"] = state_scale\n",
    "            #plt.figure() #uncomment to plot\n",
    "            #plt.xlim(0, 3*mean)  # Adjust the values as needed\n",
    "            #plt.ylim(0, 1)  # Adjust the values as needed\n",
    "            #plt.hist(group_df[state + \"Time\"], bins=5000, edgecolor='black', density=True)\n",
    "            # Plot the fitted exponential distribution\n",
    "            #x = np.sort(group_df[state + \"Time\"])\n",
    "            #plt.plot(x, exponential_fit(x, scale = state_scale), 'r-', label='Exponential Fit')\n",
    "            #plt.xlabel(f'Duration {state + \"Time\"}')\n",
    "            #plt.ylabel('Frequency')\n",
    "            #plt.title(f'{state}Time {group_name}')\n",
    "            #plt.legend()\n",
    "            #plt.show()\n",
    "\n",
    "\n",
    "            \n",
    "#MARKOV CHAIN MODELING\n",
    "print(\"Starting Markov chain modeling\")\n",
    "df = delta_df[delta_df[\"Reason\"] != \"Shutdown\" ] #remove shutdowns \n",
    "\n",
    "markov_d= {}\n",
    "#Create a list containing tuples rapresenting all possible state changes (x,y) := x --> y\n",
    "#data is a dataframe containing the correct \"UpTime\" between the states considered\n",
    "\n",
    "GenTypeGroup_df = df.groupby(\"ProductionType\")\n",
    "\n",
    "#We can use different states\n",
    "#print markov chain for each type of generator\n",
    "for production_type, data in GenTypeGroup_df:\n",
    "    transitions_probs = get_markov_probs(data, \"Type\")\n",
    "    markov_d[production_type] = transitions_probs\n",
    "    #uncomment to plot:\n",
    "    #markov_graph(transitions_probs, title = f\"{production_type} Markov Chain\")\n",
    "\n",
    "#t_probs = dict(transitions_probs)\n",
    "#for key, value in transitions_probs.items():\n",
    "#    if value < 0.05:\n",
    "#        del t_probs[key]\n",
    "#print graph\n",
    "\n",
    "\n",
    "#CAPACITY MODELING\n",
    "print(\"Starting Capacity Modeling\")\n",
    "#With Kernel density estimation\n",
    "grouped_df = df.groupby([\"ProductionType\", \"Type\"])\n",
    "capacity_d = {}\n",
    "#idea: calculate prob for capacity = 0, remove and the use non parametric fit.\n",
    "for production_type, prod_df in grouped_df:\n",
    "    #calculate per unit available capacity\n",
    "    PU = prod_df[\"AvailableCapacity\"].copy() / prod_df[\"InstalledCapacity\"]\n",
    "    df.loc[(df[\"ProductionType\"] == production_type[0]) & (df[\"Type\"] == production_type[1]), \"p.u.\"] = PU\n",
    "    PU = PU[~np.isnan(PU)]\n",
    "    p_0 = np.sum(PU == 0) / len(PU)\n",
    "    PUplus = np.sort(PU[PU != 0]) #non zero capacities\n",
    "    if len(PUplus) != 0:\n",
    "        \n",
    "        colors = [\"r\"]\n",
    "        kernels = [\"gaussian\"]\n",
    "        lw = 2\n",
    "        #Uncomment to plot\n",
    "        \n",
    "        #plt.figure()\n",
    "        for color, kernel in zip(colors, kernels):\n",
    "            kde = KernelDensity(kernel=kernel, bandwidth=0.05).fit(PUplus[:, np.newaxis])\n",
    "            capacity_d[production_type] = (p_0, kde)\n",
    "            log_dens = kde.score_samples(PUplus[:, np.newaxis]) * len(PUplus) / len(PU) #scale to be prob on non zero values\n",
    "            Y = np.exp(log_dens)\n",
    "            #plt.plot(PUplus, Y, color = color, label = kernel)\n",
    "            \n",
    "        \"\"\"\n",
    "        # Plot histogram\n",
    "\n",
    "        plt.xlim(0, 1)\n",
    "        \n",
    "        plt.hist(PU, bins=min(500, int(np.ceil(len(PU) ** (1. / 3))*3/2)) + 3 , edgecolor='black', alpha=0.7, density = True)\n",
    "\n",
    "        # Customize the plot (add labels, title, etc.)\n",
    "        plt.xlabel('p.u. Available Capacity')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'{production_type}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_gen_types = geo[\"Technology\"].unique()\n",
    "geo_fuel_types = geo[\"Fueltype\"].unique()\n",
    "grouped  = df.groupby([\"UnitName\"]).first().reset_index()\n",
    "entsoe_gen_types = list(grouped[\"ProductionType\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fffd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Scenarios\n",
    "\n",
    "#Starting state --> we roll random running time for every generator the end\n",
    "# todo: make the markov chaing run for a while to make the various generators get further in states\n",
    "#np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) \n",
    "grouped  = df.groupby([\"UnitName\"]).first().reset_index()\n",
    "gen_names = list(geo[\"projectID\"])\n",
    "gen_types = []\n",
    "for index, geo_row in geo.iterrows():\n",
    "    gen_type = get_gen_type(geo_row)\n",
    "    if gen_type is None:\n",
    "        tech = geo_row[\"Technology\"]\n",
    "        fuel = geo_row[\"Fueltype\"]\n",
    "        print(\"Weird thing:\", tech, fuel)\n",
    "        gen_types.append(\"Other \")\n",
    "    else:\n",
    "        gen_types.append(gen_type)\n",
    "\n",
    "n_gens = len(gen_names)\n",
    "pre_run_hours = 6*30*24 #time the chain is run for before producing data\n",
    "\n",
    "states = [\"Running\", \"Forced\", \"Planned\"]\n",
    "state_df = pd.DataFrame({\"UnitName\": gen_names, \"ProductionType\": gen_types, \"State\":[\"Running\"]*n_gens, \"Counter\":[0]*n_gens, \"Capacity\":[1]*n_gens})\n",
    "perc = np.ceil(pre_run_hours / 100)\n",
    "\n",
    "for h in np.arange(pre_run_hours):\n",
    "    \n",
    "    if h % (perc) == 0:\n",
    "        print(f\"{h/pre_run_hours *100} %\")\n",
    "        print(state_df.head(5))\n",
    "    \n",
    "    for index, gen_row in state_df[state_df[\"Counter\"] == 0].iterrows():\n",
    "        gen_name = gen_row[\"UnitName\"] #remove?\n",
    "        gen_type = gen_row[\"ProductionType\"]\n",
    "        current_state = gen_row[\"State\"]\n",
    "        markov = markov_d[gen_type] #get associate markov chain\n",
    "        \n",
    "        new_state = next_state_markov(markov, current_state) #get new state of the generator\n",
    "        \n",
    "        scale = statetime_df.loc[statetime_df[\"ProductionType\"] == gen_type, new_state + \"Time\"]\n",
    "        new_counter = np.ceil(np.random.exponential(scale, 1))[0] #get number of hours spent in new_state\n",
    "        \n",
    "        #get the capacity of the generator in the currnent state\n",
    "        if new_state == \"Running\":\n",
    "            new_capacity = 1\n",
    "        elif (gen_type, new_state) in capacity_d.keys():\n",
    "            p_zero, pu_pdf = capacity_d[(gen_type, new_state)]\n",
    "            if np.random.random_sample(1)[0] <= p_zero:\n",
    "                new_capacity = 0\n",
    "            else:\n",
    "                new_capacity = pu_pdf.sample(1)[0][0]\n",
    "                if new_capacity < 0:\n",
    "                    new_capacity = 0\n",
    "                elif new_capacity > 1:\n",
    "                    new_capacity = 1\n",
    "        else:\n",
    "            #todo this should not happen\n",
    "            #print(f\"no capacity distribution for {gen_type}\")\n",
    "            new_capacity = 1\n",
    "        \n",
    "        state_df.loc[index, [\"State\", \"Counter\", \"Capacity\"]] = [new_state, new_counter, new_capacity]\n",
    "        \n",
    "    state_df.loc[state_df[\"Counter\"] != 0, \"Counter\"] -= 1\n",
    "    \n",
    "#idea: \n",
    "# while t < 10 years roll dices take final state and final counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Scenarios\n",
    "# given a dataframe having UnitName and UnitType create scenarios for each UnitName\n",
    "# \n",
    "\n",
    "\n",
    "tot_hours = (end_time - start_time) / np.timedelta64(1, \"h\")\n",
    "data_list = []\n",
    "current_time = start_time\n",
    "#state_df = pd.DataFrame({\"UnitName\": gen_names, \"ProductionType\": gen_types, \"State\":[\"Running\"]*n_gens, \"Counter\":[0]*n_gens, \"Capacity\":[1]*n_gens})\n",
    "\n",
    "perc = np.ceil(tot_hours / 100)\n",
    "columns = [\"TimeStamp\"] + gen_names\n",
    "\n",
    "for h in np.arange(tot_hours):\n",
    "    \n",
    "    if h % (10*perc) == 0:\n",
    "        print(f\"{h/tot_hours *100} %\")\n",
    "        print(state_df.head(5))\n",
    "    \n",
    "    for index, gen_row in state_df[state_df[\"Counter\"] == 0].iterrows():\n",
    "        gen_name = gen_row[\"UnitName\"] #remove?\n",
    "        gen_type = gen_row[\"ProductionType\"]\n",
    "        current_state = gen_row[\"State\"]\n",
    "        markov = markov_d[gen_type] #get associate markov chain\n",
    "        \n",
    "        new_state = next_state_markov(markov, current_state) #get new state of the generator\n",
    "        \n",
    "        scale = statetime_df.loc[statetime_df[\"ProductionType\"] == gen_type, new_state + \"Time\"]\n",
    "        new_counter = np.ceil(np.random.exponential(scale, 1))[0] #get number of hours spent in new_state\n",
    "        \n",
    "        #get the capacity of the generator in the currnent state\n",
    "        if new_state == \"Running\":\n",
    "            new_capacity = 1\n",
    "        elif (gen_type, new_state) in capacity_d.keys():\n",
    "            p_zero, pu_pdf = capacity_d[(gen_type, new_state)]\n",
    "            if np.random.random_sample(1)[0] <= p_zero:\n",
    "                new_capacity = 0\n",
    "            else:\n",
    "                new_capacity = pu_pdf.sample(1)[0][0]\n",
    "                if new_capacity < 0:\n",
    "                    new_capacity = 0\n",
    "                elif new_capacity > 1:\n",
    "                    new_capacity = 1\n",
    "        else:\n",
    "            #todo this should not happen\n",
    "            #print(f\"no capacity distribution for {gen_type}\")\n",
    "            new_capacity = 1\n",
    "        \n",
    "        state_df.loc[index, [\"State\", \"Counter\", \"Capacity\"]] = [new_state, new_counter, new_capacity]\n",
    "        \n",
    "    state_df.loc[state_df[\"Counter\"] != 0, \"Counter\"] -= 1\n",
    "    current_time = current_time + np.timedelta64(1, \"h\") #move forward one hour\n",
    "    new_row = [current_time] + list(state_df[\"Capacity\"])\n",
    "    row_d = dict(zip(columns, new_row))\n",
    "    data_list.append(row_d)\n",
    "\n",
    "scenario = pd.DataFrame(data_list)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9c167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
